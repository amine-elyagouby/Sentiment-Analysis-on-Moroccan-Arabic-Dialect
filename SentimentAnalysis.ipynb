{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Sentiment Analysis on Moroccan Arabic Dialect**\n",
        "\n",
        "\n",
        "> Mohamed Amine EL YAGOUBY\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Hh3kq8-JzX5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d8DIKiUH-uS",
        "outputId": "93b3d035-3ad6-4867-f315-93e92c64c989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.9.0-py2.py3-none-any.whl (397 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/397.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/397.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.5/397.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gensim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Flatten, Dropout, LSTM, Masking,Bidirectional\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import re\n",
        "import emoji\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "ZQfV7-pcn4P3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a488c0fb-952d-435a-aa59-638a95b0a390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "SfDveoC90J45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_arabic_text(text, stop_words):\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = ''.join(c for c in text if c not in emoji.EMOJI_DATA)\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    text = ' '.join(filtered_tokens)\n",
        "    return text\n",
        "\n",
        "with open('/content/drive/MyDrive/sentiment-analysis-on-moroccan-arabic-dialect/list.txt', 'r', encoding='utf-8') as file:\n",
        "    stop_words = file.read().splitlines()\n",
        "\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/sentiment-analysis-on-moroccan-arabic-dialect/train.csv', index_col=0)\n",
        "\n",
        "data['comment'] = data['comment'].apply(lambda x: preprocess_arabic_text(x, stop_words))"
      ],
      "metadata": {
        "id": "NI8Ek5pwH2_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('arabic'))\n",
        "\n",
        "def remove_diacritics(text):\n",
        "    arabic_diacritics = re.compile(\"\"\" ّ    | # Tashdid\n",
        "                             َ    | # Fatha\n",
        "                             ً    | # Tanwin Fath\n",
        "                             ُ    | # Damma\n",
        "                             ٌ    | # Tanwin Damm\n",
        "                             ِ    | # Kasra\n",
        "                             ٍ    | # Tanwin Kasr\n",
        "                             ْ    | # Sukun\n",
        "                             ـ     # Tatwil/Kashida\n",
        "                         \"\"\", re.VERBOSE)\n",
        "    text = re.sub(arabic_diacritics, '', str(text))\n",
        "    return text\n",
        "\n",
        "def remove_emoji(text):\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "    return regrex_pattern.sub(r'',text)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
        "    text = remove_emoji(text)\n",
        "    text = remove_diacritics(text)\n",
        "    text = re.sub(r'[.,\\'\"()؛!؟“”‘’….]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    text = ' '.join([word for word in tokens if word not in stop_words])\n",
        "    return text\n",
        "data['comment'] = data['comment'].apply(clean_text)"
      ],
      "metadata": {
        "id": "NK0ohwGl4ZiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data['comment'], data['label'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "2eI4uAdMCL1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Embeddings with Word2Vec"
      ],
      "metadata": {
        "id": "qk8hYYQ-0srE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the data\n",
        "tokenized_comments = [word_tokenize(comment) for comment in X_train]\n",
        "tokenized_test_comments = [word_tokenize(comment) for comment in X_test]"
      ],
      "metadata": {
        "id": "5KAXGaEq3Hwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model = gensim.models.Word2Vec(sentences=tokenized_comments + tokenized_test_comments,\n",
        "                                        vector_size=200,  # Set the size of the word vectors\n",
        "                                        window=5,\n",
        "                                        min_count=3,\n",
        "                                        workers=4)\n",
        "\n",
        "# Training the Word2Vec model\n",
        "word2vec_model.train(X_train, total_examples=len(X_train), epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJ0-zl9G1w_Z",
        "outputId": "440bb957-9934-4139-94f7-1f88ef099353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1700, 25483500)"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Some words from the vocabulary:\")\n",
        "print(list(word2vec_model.wv.index_to_key[:100]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5pRzGBp3BzJ",
        "outputId": "1325a29b-2593-4e02-fcc6-cc0a535d0de8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some words from the vocabulary:\n",
            "['التلقيح', 'اللقاح', 'الله', 'الناس', 'لقاح', 'جواز', 'تلقيح', 'هاد', 'المغرب', 'انا', 'راه', 'الجرعة', 'كورونا', 'الدولة', 'يجب', 'الفيروس', 'الملقحين', 'باش', 'اللقاحات', 'المغاربة', 'اللي', 'الصحة', 'المناعة', 'لله', 'الشعب', 'المواطنين', 'للتلقيح', 'ديال', 'شي', 'فرض', 'الجواز', 'العالم', 'الثالثة', 'الوباء', 'اللجنة', 'شيء', 'غادي', 'الحمد', 'الحكومة', 'الدول', 'واش', 'والله', 'علينا', 'المواطن', 'ملقح', 'حنا', 'بدون', 'يحمي', 'ماشي', 'هدا', 'الملقح', 'علاش', 'العلمية', 'العدوى', 'الملك', 'مناعة', 'لينا', 'يعني', 'جونسون', 'الجماعية', 'جرعة', 'فعال', 'حالة', 'الصيني', 'الجميع', 'سينوفارم', 'العمومية', 'عليكم', 'عملية', 'الغير', 'المغربي', 'الطبيعية', 'الصحي', 'رغم', 'عليهم', 'الأولى', 'المرض', 'لان', 'صحة', 'ملقحين', 'كلشي', 'كون', 'كوفيد', 'بالتلقيح', 'الصحية', 'البلاد', 'الحق', 'حاجة', 'بلا', 'ناس', 'المسؤولية', 'شخص', 'قرار', 'بالنسبة', 'فايزر', 'خطيرة', 'جانبية', 'اجباري', 'فعالية', 'للقاح']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_seq = [\n",
        "    [word2vec_model.wv.key_to_index[word] for word in tokenized_comment if word in word2vec_model.wv.key_to_index]\n",
        "    for tokenized_comment in tokenized_comments\n",
        "]\n",
        "\n",
        "\n",
        "# Pad sequences to have a consistent length\n",
        "X_train_seq = pad_sequences(X_train_seq, maxlen=max(len(comment) for comment in tokenized_comments), padding='post')"
      ],
      "metadata": {
        "id": "TYr6Fy8rn6bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming tokenized_test_comments is a list of lists where each inner list contains tokenized words\n",
        "X_test_seq = [\n",
        "    [word2vec_model.wv.key_to_index[word] for word in tokenized_comment if word in word2vec_model.wv.key_to_index]\n",
        "    for tokenized_comment in tokenized_test_comments\n",
        "]\n",
        "\n",
        "X_test_seq = pad_sequences(X_test_seq, maxlen=max(len(comment) for comment in tokenized_comments), padding='post')"
      ],
      "metadata": {
        "id": "IEyO2hD-oYyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Building and Training"
      ],
      "metadata": {
        "id": "hDS5X7sF1CFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm = Sequential()\n",
        "\n",
        "model_lstm.add(Embedding(input_dim=len(word2vec_model.wv.index_to_key) + 1,\n",
        "                         output_dim=100,\n",
        "                         input_length=X_train_seq.shape[1],\n",
        "                         mask_zero=True))\n",
        "\n",
        "model_lstm.add(Bidirectional(LSTM(32, return_sequences=True, kernel_regularizer=l2(0.01))))\n",
        "model_lstm.add(Flatten())\n",
        "model_lstm.add(Dense(24, activation='relu'))\n",
        "model_lstm.add(Dropout(0.7))\n",
        "model_lstm.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "nRMWSwsYszhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "YcapzTpHv0cZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm.fit(X_train_seq, y_train, epochs=5, batch_size=64,validation_data = (X_test_seq,y_test) ,verbose=1)"
      ],
      "metadata": {
        "id": "xE5rOP-EoVv4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c0766a0-eca4-4056-8e58-3188df03812e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "24/24 [==============================] - 9s 378ms/step - loss: 0.3050 - accuracy: 0.9603 - val_loss: 1.1733 - val_accuracy: 0.7708\n",
            "Epoch 2/5\n",
            "24/24 [==============================] - 8s 321ms/step - loss: 0.2483 - accuracy: 0.9707 - val_loss: 1.0774 - val_accuracy: 0.7604\n",
            "Epoch 3/5\n",
            "24/24 [==============================] - 10s 412ms/step - loss: 0.1701 - accuracy: 0.9805 - val_loss: 1.1955 - val_accuracy: 0.7812\n",
            "Epoch 4/5\n",
            "24/24 [==============================] - 7s 307ms/step - loss: 0.1367 - accuracy: 0.9818 - val_loss: 1.4093 - val_accuracy: 0.7917\n",
            "Epoch 5/5\n",
            "24/24 [==============================] - 9s 384ms/step - loss: 0.1056 - accuracy: 0.9831 - val_loss: 1.5389 - val_accuracy: 0.7865\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ebfcc20fdf0>"
            ]
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Prediction"
      ],
      "metadata": {
        "id": "YZcPONZw1L4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    'لقد كان يومًا رائعًا',\n",
        "    'أنا حزين جدًا',\n",
        "    'لقد فزت بالمباراة',\n",
        "    'لقد خسرت وظيفتي',\n",
        "    'لقد تخرجت من الجامعة',\n",
        "    'لقد تزوجت',\n",
        "    'لقد أنجبت طفلًا',\n",
        "    'لقد مات والدي',\n",
        "    'لقد انتقلت إلى منزل جديد'\n",
        "]\n",
        "\n",
        "preprocessed_sentences = [preprocess_arabic_text(sentence, stop_words) for sentence in sentences]\n",
        "tokenized_sentences = [word_tokenize(sentence) for sentence in preprocessed_sentences]\n",
        "\n",
        "sequences = [\n",
        "    [word2vec_model.wv.key_to_index[word] for word in tokenized_sentence if word in word2vec_model.wv.key_to_index]\n",
        "    for tokenized_sentence in tokenized_sentences\n",
        "]\n",
        "\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max(len(comment) for comment in tokenized_comments), padding='post')\n",
        "\n",
        "predictions = model_lstm.predict(padded_sequences)\n",
        "predictions = np.round(predictions)\n",
        "\n",
        "for sentence, prediction in zip(sentences, predictions):\n",
        "    print(sentence, ':', prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABlkCgxGIt3V",
        "outputId": "56899aaa-15af-44a2-c464-334c5548a382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 6s 6s/step\n",
            "لقد كان يومًا رائعًا : [1.]\n",
            "أنا حزين جدًا : [1.]\n",
            "لقد فزت بالمباراة : [1.]\n",
            "لقد خسرت وظيفتي : [1.]\n",
            "لقد تخرجت من الجامعة : [1.]\n",
            "لقد تزوجت : [1.]\n",
            "لقد أنجبت طفلًا : [1.]\n",
            "لقد مات والدي : [0.]\n",
            "لقد انتقلت إلى منزل جديد : [1.]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}